---
title: "PracticalMachineLearning_Project"
author: "wenlarry"
date: "2/2/2017"
output: word_document
---
**Executive Summary**
The goal is to predict 5 diffferent types of exercise from accelerometers on the belt, forearm, arm and dumbell of 6 paricipants. More information is available from the website here:

http://groupware.les.inf.puc-rio.br/har

After exploring the data, we decide to use random forest to build a predictive model. The model was then cross validated with 99.3% accuracy. Then the model was used to predict the validation data with a 99.2% accuracy. Finally, the model was applied to the original datasest with 100 accuracy.

**Load Libraries**
#library(caret) 
#library(randomForest)
#library(corrplot) 
#library(rpart)
#library(rpart.plot) 

**Download Data**
```{r download} 
trainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"

testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

trainFile <-"./data/pml-training.csv"
testFile <-"./data/pml-testing.csv"
if (!file.exists("./data")){dir.create("./data")
}
if(!file.exists(trainFile)) {download.file(trainUrl, destfile=trainFile, method="curl")
}
if(!file.exists(testFile)) {download.file(testUrl,
destfile=testFile, method="curl")
}
```
Place files in dataframes
```{r df}
trainDF <-read.csv("./data/pml-training.csv") 
testDF <-read.csv("./data/pml-testing.csv")  

dim(trainDF) 
## [1] 19622 obs 160 variables
dim(testDF) 
## [1] 20 obs 160 variables
```
The classe variable in the training set is the prediction outcome.

**Data Cleaning**

Using the (str) command on the training dataframe, we
identify that there are missing values and information
that are not needed for prediction.

1) Remove missing values in columns.
```{r NA]}
trainDF <-trainDF[,colSums(is.na(trainDF))==0] 
testDF <-testDF[,colSums(is.na(testDF))==0] 
```
2) The data set contains three types of metrics.

   a) Raw metrics from the sensors
   b) Derived metrics for roll, pitch and yaw
   c) Summaries (min, max. etc) of the derived metrics
   
The summaries are not reported for every observation. As such, these 'summaries' metrics shall be excluded from this analysis, leaving just the raw and derived metrics.

In addition, the identifier fields are removed. These include row numbers, window ids and time stamps.
```{r clean} 
classe <-trainDF$classe 
trainClean <- grepl("^X|timestamp|window", names(trainDF)) 
trainDF <-trainDF[, !trainClean] 
trainClean2<-trainDF[, sapply(trainDF, is.numeric)]
trainClean2$classe<-classe 
testClean <-grepl("^X|timestamp|window", names(testDF)) 
testDF <-testDF[, !testClean]
testClean2 <- testDF[, sapply(testDF,is.numeric)] 

dim(trainClean2) 
# [1] 19622 obs 53 variables 
dim(testClean2) 
# [1] 20 obs 53 variables 
```

**Data Exploration**

Explore correlation of variables and plotting few variables, we observe that there are variables with high correlation.
```{r corVar} 
corVar <- cor(trainClean2[, -53], use="pair") 
```
```{r corvarplot, echo=FALSE}
library(corrplot) 
corrplot(corVar[1:10,1:10])  
```

Although there are high correlation of variables, we will not remove them, rather we use Random Forest as our machine learning algorithm. Random Forest is robust in dealing with correlated covariates. 

Decision Tree
```{r tree} 
library(rpart) 
tree<-rpart(classe~.,data=trainClean2, method="class") 
```
```{r treeplot,echo=FALSE} 
library(rpart.plot) 
prp (tree) 
```

**Model Building**

1) Split the cleaned training set into training data (70%) and validation data (30%) 
```{r model}
library(caret) 
set.seed (2317)
inTrain<-createDataPartition(trainClean2$classe,p=0.70,list=F) 
trainDat <- trainClean2[inTrain,]
testDat <-trainClean2[-inTrain,] 
```
2) Use Random Forest to fit a predictive model with
a conventional 10 fold cross validation. Our 
data exploration revealed that accuracy converged after about 100 trees.
```{r rf}
rfControl <-trainControl(method="cv", 10) 
rfModel <- train(classe~., data=trainDat,method="rf",
trControl=rfControl,ntree=100) 
rfModel
```
```{r rfplot, echo=FALSE} 
plot (rfModel) 
```
The final value used by the model was mtry = 27 with 99.2% accuracy. It has 52 randomly selected predictors from a 10 fold cross validation. The out of sample error would be 0.8% (o.e.1-.0992)

3) Predictions on the validation data
```{r pred} 
rfPredict<-predict(rfModel,testDat) 
confusionMatrix(testDat$classe, rfPredict) 

accuracy <- postResample( rfPredict,testDat$classe) 
accuracy

outSam <- 1-(confusionMatrix(testDat$classe,rfPredict)$overall[1]) 
outSam
```
The estimated accuracy of the model is 99.3%
the estmated out-of sample error is 0.7% 

**Test Dataset Prediction**

Apply the model to the original testing data after 
removing the column 'problem_id'
```{r pred2}
Testdata <- predict(rfModel, testClean2[,-length(names(testClean2))] ) 
Testdata
```
A 100% accuracy
